# Modelos disponibles en Groq (Actualizados)

## üöÄ Modelos recomendados (2025)

### LLaMA 3 70B (Recomendado)
```typescript
model: groq("llama3-70b-8192")
```
- **Mejor para**: Generaci√≥n de contenido complejo, razonamiento avanzado
- **Pros**: M√°s inteligente, mejores respuestas, 8K tokens de contexto
- **Contras**: Un poco m√°s lento que modelos menores
- **Uso recomendado**: Proyectos que requieren alta calidad

### LLaMA 3 8B Instant
```typescript
model: groq("llama3-8b-8192")
```
- **Mejor para**: Respuestas r√°pidas, tareas simples
- **Pros**: Ultra-r√°pido, eficiente, 8K tokens
- **Contras**: Menos sofisticado que el 70B
- **Uso recomendado**: Prototipos, desarrollo, aplicaciones de alta velocidad

### Mixtral 8x7B
```typescript
model: groq("mixtral-8x7b-32768")
```
- **Mejor para**: Balance entre velocidad y calidad
- **Pros**: Buen balance, maneja contextos largos (32k tokens)
- **Contras**: No tan avanzado como LLaMA 3 70B
- **Uso recomendado**: An√°lisis de documentos largos

### Gemma 7B
```typescript
model: groq("gemma-7b-it")
```
- **Mejor para**: Tareas espec√≠ficas, respuestas estructuradas
- **Pros**: R√°pido, eficiente para tareas espec√≠ficas
- **Contras**: Menos vers√°til que LLaMA
- **Uso recomendado**: Casos de uso espec√≠ficos

## üîÑ C√≥mo cambiar el modelo

1. Abre `app/api/generate-questions/route.ts`
2. Busca la l√≠nea:
```typescript
model: groq("llama-3.1-70b-versatile"),
```
3. Reempl√°zala con el modelo que prefieras

## üìä Comparaci√≥n de rendimiento

| Modelo | Velocidad | Calidad | Contexto | Recomendado para |
|--------|-----------|---------|----------|------------------|
| LLaMA 3 70B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8K tokens | Producci√≥n, calidad |
| LLaMA 3 8B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 8K tokens | Desarrollo, velocidad |
| Mixtral 8x7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 32K tokens | Documentos largos |
| Gemma 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 4K tokens | Tareas espec√≠ficas |

## üéØ Consejos de uso

### Para preguntas de examen (nuestro caso):
- **Recomendado**: `llama3-70b-8192`
- **Alternativa r√°pida**: `llama3-8b-8192`

### Para an√°lisis de PDFs largos:
- **Recomendado**: `mixtral-8x7b-32768`

### Para desarrollo/testing:
- **Recomendado**: `llama-3.1-8b-instant`

## üîß Configuraci√≥n avanzada

Tambi√©n puedes configurar par√°metros adicionales:

```typescript
const { text } = await generateText({
  model: groq("llama3-70b-8192"),
  temperature: 0.7,        // Creatividad (0.0 - 2.0)
  maxTokens: 2000,         // M√°ximo de tokens en respuesta
  topP: 0.9,              // Diversidad de palabras
  stop: ["END"],          // Palabras que detienen la generaci√≥n
  prompt: "Tu prompt aqu√≠..."
})
```

### Par√°metros recomendados para preguntas de examen:
```typescript
temperature: 0.3,        // Baja creatividad para precisi√≥n
maxTokens: 1500,         // Suficiente para 5 preguntas
topP: 0.8,              // Balance entre diversidad y consistencia
```
